"""
Universal Product Scraper
H·ªó tr·ª£ tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m t·ª´ nhi·ªÅu lo·∫°i website kh√°c nhau
"""

import re
import json
import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
import time
import random
import html
import asyncio
# from utils.crawl_request_html import crawl_webpage as async_crawl_webpage

def get_useragent():
    """
    Generates a random user agent string mimicking the format of various software versions.

    The user agent string is composed of:
    - Lynx version: Lynx/x.y.z where x is 2-3, y is 8-9, and z is 0-2
    - libwww version: libwww-FM/x.y where x is 2-3 and y is 13-15
    - SSL-MM version: SSL-MM/x.y where x is 1-2 and y is 3-5
    - OpenSSL version: OpenSSL/x.y.z where x is 1-3, y is 0-4, and z is 0-9

    Returns:
        str: A randomly generated user agent string.
    """
    lynx_version = (
        f"Lynx/{random.randint(2, 3)}.{random.randint(8, 9)}.{random.randint(0, 2)}"
    )
    libwww_version = f"libwww-FM/{random.randint(2, 3)}.{random.randint(13, 15)}"
    ssl_mm_version = f"SSL-MM/{random.randint(1, 2)}.{random.randint(3, 5)}"
    openssl_version = (
        f"OpenSSL/{random.randint(1, 3)}.{random.randint(0, 4)}.{random.randint(0, 9)}"
    )
    return f"{lynx_version} {libwww_version} {ssl_mm_version} {openssl_version}"


class UniversalProductScraper:
    """
    Scraper t·ªïng h·ª£p c√≥ th·ªÉ x·ª≠ l√Ω nhi·ªÅu lo·∫°i website
    """

    def __init__(self, use_llm: bool = False, llm_api_key: Optional[str] = None):
        self.use_llm = use_llm
        self.llm_api_key = llm_api_key

        # Headers ƒë·ªÉ tr√°nh b·ªã block
        self.headers = {
            # 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            "User-Agent": get_useragent(),
            # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            # 'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            "Accept": "*/*",

        }

        # C√°c pattern chung cho m·ªçi website
        self.universal_patterns = {
            "price": [
                r"(?:Gi√° [^:]*)[:\s]*([\d.,]+)|(\d{1,3}(?:\.\d{3})+\s*(?:ƒë))"
                # r"(?:VND)\s*([\d.,]+)",
                # r"([\d.,]+)\s*(?:ƒë)",
            ],
            # "name": [
            #     r"<h1[^>]*>(.*?)</h1>",
            #     r"<title>(.*?)</title>",
            # ],
        }

        # Config cho t·ª´ng website c·ª• th·ªÉ
        self.site_configs = self._load_site_configs()

    def _load_site_configs(self) -> Dict:
        """
        Config cho c√°c website ph·ªï bi·∫øn
        """
        return {
            "shopee.vn": {
                "selectors": {
                    "name": [".product-title", "h1", '[class*="product-name"]'],
                    "price": ['[class*="price"]', ".product-price"],
                    "description": [".product-description", '[class*="description"]'],
                    "images": ['img[class*="product"]', ".product-image img"],
                },
                "json_ld": True,  # H·ªó tr·ª£ JSON-LD schema
            },
            "lazada.vn": {
                "selectors": {
                    "name": [".pdp-product-title", "h1"],
                    "price": [".pdp-price", '[class*="price"]'],
                    "description": [".detail-content"],
                },
                "json_ld": True,
            },
            "tiki.vn": {
                "selectors": {
                    "name": ['h1[class*="title"]', ".product-name"],
                    "price": ['[class*="product-price"]'],
                    "rating": ['[class*="rating"]'],
                },
                "json_ld": True,
            },
            "sendo.vn": {
                "selectors": {
                    "name": [".product_name", "h1"],
                    "price": [".product_price"],
                },
            },
            # Th√™m config cho c√°c site kh√°c
            "default": {
                "selectors": {
                    "name": [
                        "h1",
                        '[itemprop="name"]',
                        ".product-title",
                        ".product-name",
                    ],
                    "price": [
                        '[itemprop="price"]',
                        ".price",
                        ".product-price",
                        '[class*="price"]',
                    ],
                    "description": [
                        '[itemprop="description"]',
                        ".description",
                        ".product-description",
                        ".summary-content"                        
                    ],
                    "images": [
                        '[itemprop="image"]',
                        ".product-image img",
                        'img[alt*="product"]',
                        'img'
                    ],
                    "sku": ['[itemprop="sku"]', ".sku", ".product-code"],
                    "brand": ['[itemprop="brand"]', ".brand"],
                },
                "json_ld": True,
            },
        }

    def _clean_redundant_text(self, text):
        # 1. T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u d·ª±a tr√™n d·∫•u ch·∫•m
        parts = [p.strip() for p in text.split(".") if p.strip()]

        unique_segments = []
        seen = set()

        for segment in parts:
            # 2. Chu·∫©n h√≥a: lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
            segment = re.sub(r"\s+", " ", segment).strip()

            # 3. Lo·∫°i b·ªè c√°c m·∫´u "R√°c" c√≥ d·∫°ng "Th√¥ng tin: ." (kh√¥ng c√≥ gi√° tr·ªã th·ª±c)
            # Regex n√†y t√¨m c√°c chu·ªói k·∫øt th√∫c b·∫±ng d·∫•u hai ch·∫•m v√† kho·∫£ng tr·∫Øng/d·∫•u ch·∫•m r·ªóng
            if re.search(r":\s*\.?$", segment):
                continue

            # 4. Ki·ªÉm tra tr√πng l·∫∑p
            if segment not in seen:
                unique_segments.append(segment)
                seen.add(segment)

        # 5. N·ªëi l·∫°i th√†nh ƒëo·∫°n vƒÉn b·∫£n ho√†n ch·ªânh
        return ". ".join(unique_segments) + "."

    def scrape(self, url: str, method: str = "auto") -> Dict[str, Any]:
        """
        Scrape th√¥ng tin s·∫£n ph·∫©m t·ª´ URL

        Args:
            url: URL c·ªßa s·∫£n ph·∫©m
            method: 'auto', 'html', 'json_ld', 'llm', 'hybrid'
        """
        print(f"üîç ƒêang scrape: {url}")

        # Fetch HTML
        html_content = self._fetch_url(url)
        # print("html_content:", html_content)
        if not html_content:
            # return {"error": "Failed to fetch URL"}
            return None

        # Parse HTML
        soup = BeautifulSoup(html_content, "html.parser")
        with open("demofile.txt", "w") as f:
            f.write(str(soup))

        # X√°c ƒë·ªãnh ph∆∞∆°ng ph√°p scrape
        if method == "auto":
            method = self._detect_best_method(soup, url)

        print(f"üìä S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p: {method}")

        # Scrape theo ph∆∞∆°ng ph√°p
        if method == "json_ld":
            result = self._extract_from_json_ld(soup)
        elif method == "llm" and self.use_llm:
            result = self._extract_with_llm(html_content)
        elif method == "hybrid":
            result = self._extract_hybrid(soup, html_content)

        else:  # html
            result = self._extract_from_html(soup, url)
            # print("result html:", result)

        print("raw result:", result)

        # Post-processing
        result = self._clean_result(result)

        final_result = False

        # if not result["currency"]:
        # result["currency"] = ""
        check_pattern = r"(\d{1,3}(?:\.\d{3})+)"

        if result:
            try:
            
                # match = re.search(check_pattern, str(result["price"]), re.IGNORECASE)
                # if match:

                        specs = self._clean_redundant_text(result["description"])

                        final_result = {
                            "name": result["name"],
                            "price": str(result["price"]),
                            "specs": specs,
                            "link": url,
                            "image": result["images"][0],
                        }
                        # return final_result
            except Exception as e:
                print("l·ªói xu·∫•t th√¥ng tin", e, url)
                final_result = False
            # else:
            #     final_result = False

        # result['url'] = url
        # result['scrape_method'] = method
        print("final_result:", final_result)

        return final_result

    def _fetch_url(self, url: str) -> Optional[str]:
        """
        Fetch HTML t·ª´ URL v·ªõi retry
        """
        max_retries = 3
        for i in range(max_retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=15)
                # Fix encoding UTF-8
                response.encoding = "utf-8"
                if response.status_code == 200:
                    return response.text

                # response = await async_crawl_webpage(url)
                # if response:
                #     return response
                
                elif response.status_code == 403:
                    print(f"‚ö†Ô∏è  B·ªã ch·∫∑n, th·ª≠ l·∫°i l·∫ßn {i + 1}...")
                    time.sleep(2)
                else:
                    print(f"‚ùå Status code: {response.status_code}, url: {url}")
                    return None
            except Exception as e:
                print(f"‚ùå Error: {e}")
                if i < max_retries - 1:
                    time.sleep(2)
        return None

    def _detect_best_method(self, soup: BeautifulSoup, url: str) -> str:
        """
        T·ª± ƒë·ªông ph√°t hi·ªán ph∆∞∆°ng ph√°p scrape t·ªët nh·∫•t
        """
        # Check JSON-LD
        json_ld = soup.find("script", type="application/ld+json")
        if json_ld:
            return "json_ld"

        # Check meta tags (Open Graph, Twitter Card)
        og_tags = soup.find_all("meta", property=re.compile(r"^og:"))
        if len(og_tags) >= 3:
            return "html"  # C√≥ ƒë·ªß meta tags

        # N·∫øu c√≥ LLM v√† page ph·ª©c t·∫°p
        if self.use_llm:
            return "hybrid"

        return "html"

    def _extract_from_json_ld(self, soup: BeautifulSoup) -> Dict:
        """
        Tr√≠ch xu·∫•t t·ª´ JSON-LD Schema (chu·∫©n Schema.org)
        ƒê√¢y l√† c√°ch t·ªët nh·∫•t n·∫øu website h·ªó tr·ª£
        """
        result = {}

        json_ld_scripts = soup.find_all("script", type="application/ld+json")
        # with open("demofile.txt", "w") as f:
        #     f.write(str(json_ld_scripts))

        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)

                # Handle array
                if isinstance(data, list):
                    data = data[0]

                # Extract Product schema
                if data.get("@type") == "Product":
                    result["name"] = data.get("name")
                    result["description"] = data.get("description")
                    # result["sku"] = data.get("sku")
                    # result["brand"] = data.get("brand", {}).get("name")

                    # Price
                    if "offers" in data:
                        offers = data["offers"]
                        if isinstance(offers, list):
                            offers = offers[0]
                        result["price"] = offers.get("price")
                        result["currency"] = offers.get("priceCurrency")
                        # result["availability"] = offers.get("availability")

                    # Images
                    if "image" in data:
                        images = data["image"]
                        if isinstance(images, str):
                            result["images"] = [images]
                        elif isinstance(images, list):
                            result["images"] = images

                    # Rating
                    if "aggregateRating" in data:
                        rating = data["aggregateRating"]
                        result["rating"] = rating.get("ratingValue")
                        result["review_count"] = rating.get("reviewCount")

            except json.JSONDecodeError:
                continue

        return result

    def _extract_from_html(self, soup: BeautifulSoup, url: str) -> Dict:
        """
        Tr√≠ch xu·∫•t t·ª´ HTML structure v√† meta tags
        """
        result = {}

        # X√°c ƒë·ªãnh config d·ª±a tr√™n domain
        domain = urlparse(url).netloc
        config = self.site_configs.get(domain, self.site_configs["default"])
        
        # Extract t·ª´ meta tags (Open Graph, Twitter Card)
        result.update(self._extract_from_meta_tags(soup))

        # Extract t·ª´ microdata
        result.update(self._extract_from_microdata(soup))

        # Extract b·∫±ng regex patterns
        html_text = soup.get_text()
        html_text = html.unescape(html_text)
        # with open("demofile.txt", "w") as f:
        #     f.write(html_text)
        for field, patterns in self.universal_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, html_text, re.IGNORECASE)
                if match:
                    try:
                        # T√¨m group ƒë·∫ßu ti√™n kh√¥ng ph·∫£i None
                        value = None
                        for group_idx in range(1, len(match.groups()) + 1):
                            if match.group(group_idx) is not None:
                                value = match.group(group_idx).strip()
                                break
                        
                        if value:
                            # print("pattern:", pattern)
                            # print("group:", value)
                            result[field] = value
                            break
                    except (IndexError, AttributeError) as e:
                        print(f"‚ùå Error extracting: {e}")
                        continue
                else:
                    print(f"‚ö†Ô∏è pattern '{pattern}' - no match")    

        # Extract theo selectors
        for field, selectors in config["selectors"].items():
            for selector in selectors:
                # print("selector:", selector)
                elements = soup.select(selector)
                if elements:
                    # print(f"elements {field}:", elements)
                    if field == "images" and result["name"]:
                        # print("elements images:", elements)
                        for img in elements:
                            # if (img.get("src") or (img.get("data-src")) and (img.get("alt") and(img.get("alt")==result["name"]))):
                            if (str(img.get("alt")) in result["name"]):
                                # print(img.get("alt") )
                                # print("name:", result["name"])
                                result[field] = [
                                    img.get("src") or img.get("data-src")
                                ]
                                break                            
                        # print("result images:", result[field])
                    else:
                        if field == "description":
                        
                            max=""
                            for element in elements:
                                if len(element.get_text(strip=True))>len(max):
                                    max=element.get_text(strip=True)
                                    # print("max:", max)
                            result[field] = str(max)
                        # else:
                        #     result[field] = elements[0].get_text(strip=True)
                        #     break                                        

        return result

    def _extract_from_meta_tags(self, soup: BeautifulSoup) -> Dict:
        """
        Tr√≠ch xu·∫•t t·ª´ Open Graph v√† Twitter Card meta tags
        """
        result = {}

        # Open Graph tags
        og_mapping = {
            "og:title": "name",
            "og:description": "description",
            "og:image": "images",
            "og:price:amount": "price",
            "og:price:currency": "currency",
        }

        for og_prop, field in og_mapping.items():
            tag = soup.find("meta", property=og_prop)
            if tag and tag.get("content"):
                if field == "images":
                    result.setdefault("images", []).append(tag["content"])
                else:
                    result[field] = tag["content"]

        # Twitter Card
        twitter_mapping = {
            "twitter:title": "name",
            "twitter:description": "description",
            "twitter:image": "images",
        }

        for tw_name, field in twitter_mapping.items():
            if field not in result or field == "images":
                tag = soup.find("meta", attrs={"name": tw_name})
                if tag and tag.get("content"):
                    # print("tag content:", tag["content"], tag.get("content"))
                    # if field == "images":
                        print("tag twitter image:", result[field], tag["content"])
                        result[field].append(tag["content"])
        print("result images twitter:", result["images"])
                    # else:
                    #     result[field] = tag["content"]

        return result

    def _extract_from_microdata(self, soup: BeautifulSoup) -> Dict:
        """
        Tr√≠ch xu·∫•t t·ª´ microdata (itemprop)
        """
        result = {}

        mapping = {
            "name": "name",
            "description": "description",
            "price": "price",
            "priceCurrency": "currency",
            "image": "image",
            "sku": "sku",
            "brand": "brand",
        }

        for itemprop, field in mapping.items():
            element = soup.find(attrs={"itemprop": itemprop})
            if element:
                if element.name == "meta":
                    value = element.get("content")
                elif element.name == "img":
                    value = element.get("src")
                else:
                    value = element.get_text(strip=True)

                if value:
                    if field == "images":
                        result[field].append(value)
                    else:
                        result[field] = value
        try:
            print("result images microdata:", result["images"])
        except Exception:
            print("no images microdata")

        return result

    def _extract_with_llm(self, html_content: str) -> Dict:
        """
        S·ª≠ d·ª•ng LLM (Claude/GPT) ƒë·ªÉ tr√≠ch xu·∫•t
        Ph∆∞∆°ng ph√°p n√†y ƒë·∫Øt nh∆∞ng ch√≠nh x√°c cao
        """
        if not self.llm_api_key:
            return {"error": "LLM API key not provided"}

        # Clean HTML
        soup = BeautifulSoup(html_content, "html.parser")
        for tag in soup(["script", "style", "nav", "footer", "header"]):
            tag.decompose()

        text = soup.get_text()
        # Limit text length to save cost
        text = text[:8000]  # ~2000 tokens

        try:
            import anthropic

            client = anthropic.Anthropic(api_key=self.llm_api_key)

            prompt = f"""
Tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m t·ª´ vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON v·ªõi c√°c tr∆∞·ªùng:
- name: T√™n s·∫£n ph·∫©m
- price: Gi√° (s·ªë v√† ƒë∆°n v·ªã)
- description: M√¥ t·∫£ ng·∫Øn
- brand: Th∆∞∆°ng hi·ªáu
- sku: M√£ s·∫£n ph·∫©m
- specifications: Th√¥ng s·ªë k·ªπ thu·∫≠t (dict)
- images: URL ·∫£nh (array)
- availability: T√¨nh tr·∫°ng c√≤n h√†ng

VƒÉn b·∫£n:
{text}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch.
"""

            message = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}],
            )

            response_text = message.content[0].text
            # Clean markdown
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0]

            return json.loads(response_text.strip())

        except Exception as e:
            print(f"‚ùå LLM extraction failed: {e}")
            return {"error": str(e)}

    def _extract_hybrid(self, soup: BeautifulSoup, html_content: str) -> Dict:
        """
        K·∫øt h·ª£p nhi·ªÅu ph∆∞∆°ng ph√°p ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao nh·∫•t
        """
        result = {}

        # 1. JSON-LD (∆∞u ti√™n cao nh·∫•t)
        json_ld_data = self._extract_from_json_ld(soup)
        result.update(json_ld_data)
        print("keys json_ld_data:", json_ld_data.keys())

        # 2. HTML structure
        html_data = self._extract_from_html(soup, "")
        for key, value in html_data.items():
            if key not in result or not result[key] or key == "price" or key == "images":
                result[key] = value

        # # 3. LLM cho c√°c tr∆∞·ªùng c√≤n thi·∫øu
        # if self.use_llm and len(result) < 5:
        #     llm_data = self._extract_with_llm(html_content)
        #     for key, value in llm_data.items():
        #         if key not in result or not result[key]:
        #             result[key] = value

        return result            

    def _clean_result(self, result: Dict) -> Dict:
        """
        L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu
        """
        cleaned = {}

        for key, value in result.items():
            if value is None or value == "":
                continue

            # Fix encoding issues
            if isinstance(value, str):
                # Try to fix common encoding problems
                try:
                    # Decode if needed
                    if "\\u" in value or "∆í" in value or "‚àö" in value:
                        # Try to fix mojibake
                        value = value.encode("latin1").decode("utf-8", errors="ignore")
                except Exception:
                    pass

                # Remove extra whitespace
                value = " ".join(value.split())
                value = value.strip()

                # decode HTML entities v√† strip tags
                decoded = html.unescape(value)
                print("decoded:", decoded)
                soup = BeautifulSoup(decoded, "html.parser")
                text = soup.get_text(separator=" ").strip()

                # lo·∫°i b·ªè k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ/s·ªë/kho·∫£ng tr·∫Øng/.,:;()-/%¬∞
                cleaned_value = re.sub(r"[^0-9A-Za-z√Ä-·ªπ√†-·ªπ\s\.,:;\(\)\-\‚Äì\/%¬∞]+", "", text)

                # g·ªôp nhi·ªÅu kho·∫£ng tr·∫Øng th√†nh m·ªôt
                cleaned_value = re.sub(r"\s+", " ", cleaned_value).strip()

                # Remove HTML tags
                value = re.sub(r"<[^>]+>", "", cleaned_value)

            # Clean price
            if key == "price" and isinstance(value, str):
                # Extract numbers
                price_match = re.search(r"([\d.,]+)", value.replace(",", ""))
                if price_match:
                    cleaned[key] = price_match.group(1)
                    # Extract currency
                    if "‚Ç´" in value or "VND" in value or "ƒë" in value:
                        cleaned["currency"] = "VND"
                else:
                    cleaned[key] = value
            else:
                cleaned[key] = value

        return cleaned

    def scrape_multiple(self, urls: List[str]) -> List[Dict]:
        """
        Scrape nhi·ªÅu URL
        """
        results = []
        for i, url in enumerate(urls, 1):
            print(f"\n{'=' * 60}")
            print(f"üì¶ S·∫£n ph·∫©m {i}/{len(urls)}")
            result = self.scrape(url)
            results.append(result)
            time.sleep(1)  # Tr√°nh b·ªã ban
        return results

    def save_results(self, results: List[Dict], filename: str = "products.json"):
        """
        L∆∞u k·∫øt qu·∫£ v√†o file
        """
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ ƒê√£ l∆∞u {len(results)} s·∫£n ph·∫©m v√†o {filename}")


# ===== USAGE EXAMPLES =====
if __name__ == "__main__":
    def main():
        # Kh·ªüi t·∫°o scraper
        scraper = UniversalProductScraper(
            use_llm=False,  # Set True n·∫øu mu·ªën d√πng LLM
            llm_api_key="your-api-key-here",  # Th√™m API key n·∫øu d√πng LLM
        )


        # Example 1: Scrape m·ªôt s·∫£n ph·∫©m
        # json ld - price 0: https://rangdong.com.vn/den-pha-led-100w-2019-pr1215.html
        url = "https://rangdong.com.vn/vot-bat-muoi-vbm-rd-03-pr2398.html"
        result = scraper.scrape(url, method="auto")  # 'auto', 'html', 'json_ld', 'llm', 'hybrid'
        print("\nüìä K·∫æT QU·∫¢:")
        print(json.dumps(result, indent=2, ensure_ascii=False))

        # Example 2: Scrape nhi·ªÅu s·∫£n ph·∫©m
        urls = [
            "https://shopee.vn/product1",
            "https://tiki.vn/product2",
            "https://lazada.vn/product3",
        ]
        # results = scraper.scrape_multiple(urls)
        # scraper.save_results(results, 'products.json')

        # Example 3: Scrape v·ªõi ph∆∞∆°ng ph√°p c·ª• th·ªÉ
        # result = scraper.scrape(url, method='json_ld')  # Ch·ªâ d√πng JSON-LD
        # result = scraper.scrape(url, method='llm')      # Ch·ªâ d√πng LLM
        # result = scraper.scrape(url, method='hybrid')   # K·∫øt h·ª£p t·∫•t c·∫£
    # asyncio.run(main())
    main()